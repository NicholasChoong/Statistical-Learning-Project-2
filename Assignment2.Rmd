---
title: "Assignment2"
author:
- Michael Nefiodovas(22969312)
- Carmen Leong(22789943)
- Nicholas Choong(21980614)
output:
  pdf_document: default
  html_notebook: default
  word_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include=FALSE}
# setwd("~/Library/CloudStorage/OneDrive-TheUniversityofWesternAustralia/STAT3064/Data Sets")
setwd("C:/Users/user/OneDrive - The University of Western Australia/Units/STAT3064/Labs/STAT3064-Ass2")

rm(list = ls())

if (!is.null(sessionInfo()$otherPkgs)) {
  invisible(
    lapply(paste0("package:", names(sessionInfo()$otherPkgs)),
      detach,
      character.only = TRUE, unload = TRUE
    )
  )
}

options(stringsAsFactors = FALSE)
```

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
library(reshape2)
library(tidyverse)
library(MASS)
library(GGally)
library(mvtnorm)
library(scales)
library(ggpubr)
library(dplyr)
library(factoextra)
library(broom)
library(pls)
```

# Question 1

## (a) Why should you not automatically scale the data prior to a PCA or FA? Restrict your answer to one or two concise sentences.

Scaling can possibly cause information loss especially for variables
with high magnitude. If the variables with high magnitude are important,
scaling is not advised since scaling centres all the variables and
transforms their variability and range to more comparable ranges.
Moreover, if the variables in the data set have same units of
measurement, scaling is not necessarily required.

## (b) The dataset ass2pop.csv is available in the LMS folder 'Data sets'. For a description of the data see Assignment 1. Here we work with a part of the dataset only. Let $\Sigma$ be the covariance matrix consisting of rows 1:11, and columns 3:13. Read the data into R. The value for $\Sigma$[1, 1] should be 0.8266. In your answer show the R commands you use to calculate the following and show the results stating clearly what each part is.

```{r}
ass2pop <- read.csv("ass2pop.csv", header = FALSE)
S0 <- as.matrix(ass2pop[1:11, 3:13])
dim(S0)
```

The covariance matrix is a 11x11 square matrix giving the covariance
between each pair of the first 11 variables from the first population.

### i. the eigenvalues of $\Sigma$;

```{r}
ev <- eigen(S0)
vals <- ev$values
knitr::kable(round(vals, digits = 4))
```

The eigenvalues of the covariance matrix encode the variability of the
data in an orthogonal basis that captures as much of the data's
variability as possible.

### ii. the matrix $\Sigma^{2/3}$;

```{r}
V1 <- ev$vectors
vals2 <- diag(vals^(2 / 3))
S1 <- V1 %*% vals2 %*% t(V1)
knitr::kable(round(S1, digits = 4))
```

### iii. the matrix $2\Sigma^{-1/4}$$\Sigma$$\Sigma^{-1/4}$and its eigenvalues

```{r}
vals3 <- diag(vals^(-1 / 4))
S2 <- V1 %*% vals3 %*% t(V1)
mat <- 2 * S2 %*% S0 %*% S2
knitr::kable(round(mat, digits = 4))
vals4 <- eigen(mat)$values
vals4
```

# Question 2

## Consider the abalone data. We want to compare the performance of linear regression and PCR for the raw abalone data following the description given in Q3 of Lab 3. In the analysis we use the predictor variables Length, Height, Whole Weight, Shucked Weight, Viscera Weight and Dried-Shell Weight and we consider Rings as the response variable. Hint. Note the change of predictor variables used in Q2 compared to the variables in the Lab.

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
coln <- c(
  "Sex", # 		nominal			M, F, and I (infant)
  "Length", # 		continuous	mm	Longest shell measurement
  "Diameter", # 	continuous	mm	perpendicular to length
  "Height", # 		continuous	mm	with meat in shell
  "Whole_weight", # 	continuous	grams	whole abalone
  "Shucked_weight", # 	continuous	grams	weight of meat
  "Viscera_weight", # 	continuous	grams	gut weight (after bleeding)
  "Shell_weight", # 	continuous	grams	after being dried
  "Rings" # 		integer			+1.5 gives the age in years
)
abalone <- read_csv(file = "./abalone.csv", col_names = coln)
abalone$Sex <- as.factor(abalone$Sex)
summary(abalone)
```

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
# Regression
big.lm <- lm(Rings ~ Length + Height + Whole_weight + Shucked_weight + Viscera_weight + Shell_weight, data = abalone)
summary(big.lm)
```

### (a) For the regular linear regression use forward selection and state the order in which the variables are chosen. Calculate the residual standard deviation for each number of predictors. Hint. you may make use of the code in Lab 3.

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
glancerows <- data.frame()
fm.fwd <- fm.null <- lm(Rings ~ 1, abalone)
summary(lm(fm.fwd))
row1 <- data.frame(modelno = 0, variable = "(Intercept)", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
fm.fwd <- update(fm.fwd, . ~ . + Shell_weight)
row1 <- data.frame(modelno = 1, variable = "Shell_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
fm.fwd <- update(fm.fwd, . ~ . + Shucked_weight)
row1 <- data.frame(modelno = 2, variable = "Shucked_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
fm.fwd <- update(fm.fwd, . ~ . + Length)
row1 <- data.frame(modelno = 3, variable = "Length", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
fm.fwd <- update(fm.fwd, . ~ . + Whole_weight)
row1 <- data.frame(modelno = 4, variable = "Whole_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
fm.fwd <- update(fm.fwd, . ~ . + Height)
row1 <- data.frame(modelno = 5, variable = "Height", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
fm.fwd <- update(fm.fwd, . ~ . + Viscera_weight)
row1 <- data.frame(modelno = 6, variable = "Viscera_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
```

```{r, echo=FALSE, results='show', message=FALSE, warning=FALSE, fig.show='hide'}
glancerows1 <- data.frame(modelno = glancerows$modelno, sigma = glancerows$sigma)
rownames(glancerows1) <- glancerows$variable
knitr::kable(glancerows1)
```

Shell_weight, Shucked_weight, Length, Whole_weight, Height,
Viscera_weight

### (b) Carry out PCR on the raw data using the same variables and response as in part (a). For each additional principal component you add to the regression model as predictor, calculate the residual standard deviation and list which of the variables has the highest absolute weight in the respective principal component.

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
abalone1 <- dplyr::select(
  abalone,
  Shell_weight,
  Shucked_weight,
  Length,
  Whole_weight,
  Height,
  Viscera_weight
)
abalone_pr <- prcomp(abalone1, scale = F)

abalone_lm_tmp <- cbind(data.frame(Rings = abalone$Rings), abalone_pr$x)
glancerows_pc <- data.frame()
abalone_lm <- lm(Rings ~ 1, abalone_lm_tmp)
row1_pc <- data.frame(modelno = 0, variable = "(Intercept)", sigma = glance(abalone_lm)$sigma)
glancerows_pc <- rbind(glancerows_pc, row1_pc)

abalone_lm <- lm(Rings ~ PC1, abalone_lm_tmp)
row1_pc <- data.frame(modelno = 1, variable = "1 comps", sigma = glance(abalone_lm)$sigma)
glancerows_pc <- rbind(glancerows_pc, row1_pc)

abalone_lm <- lm(Rings ~ PC1 + PC2, abalone_lm_tmp)
row1_pc <- data.frame(modelno = 2, variable = "2 comps", sigma = glance(abalone_lm)$sigma)
glancerows_pc <- rbind(glancerows_pc, row1_pc)

abalone_lm <- lm(Rings ~ PC1 + PC2 + PC3, abalone_lm_tmp)
row1_pc <- data.frame(modelno = 3, variable = "3 comps", sigma = glance(abalone_lm)$sigma)
glancerows_pc <- rbind(glancerows_pc, row1_pc)

abalone_lm <- lm(Rings ~ PC1 + PC2 + PC3 + PC4, abalone_lm_tmp)
row1_pc <- data.frame(modelno = 4, variable = "4 comps", sigma = glance(abalone_lm)$sigma)
glancerows_pc <- rbind(glancerows_pc, row1_pc)

abalone_lm <- lm(Rings ~ PC1 + PC2 + PC3 + PC4 + PC5, abalone_lm_tmp)
row1_pc <- data.frame(modelno = 5, variable = "5 comps", sigma = glance(abalone_lm)$sigma)
glancerows_pc <- rbind(glancerows_pc, row1_pc)

abalone_lm <- lm(Rings ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6, abalone_lm_tmp)
row1_pc <- data.frame(modelno = 6, variable = "6 comps", sigma = glance(abalone_lm)$sigma)
glancerows_pc <- rbind(glancerows_pc, row1_pc)
```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='show'}
ggcorr(abalone1, label = T) +
  ggtitle("Heatmap of Predictor Variables")
```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='show'}
pc1 <- fviz_contrib(abalone_pr, choice = "var", axes = 1) +
  ggtitle("Contribution of variables to PC1")

pc2 <- fviz_contrib(abalone_pr, choice = "var", axes = 2) +
  ggtitle("Contribution of variables to PC2")

pc3 <- fviz_contrib(abalone_pr, choice = "var", axes = 3) +
  ggtitle("Contribution of variables to PC3")

pc4 <- fviz_contrib(abalone_pr, choice = "var", axes = 4) +
  ggtitle("Contribution of variables to PC4")

pc5 <- fviz_contrib(abalone_pr, choice = "var", axes = 5) +
  ggtitle("Contribution of variables to PC5")

pc6 <- fviz_contrib(abalone_pr, choice = "var", axes = 6) +
  ggtitle("Contribution of variables to PC6")

figure <- ggarrange(pc1, pc2, pc3, pc4, pc5, pc6,
  ncol = 2, nrow = 3
)
figure
```

```{r, echo=FALSE, results='show', message=FALSE, warning=FALSE, fig.show='hide'}
var_contribution <- c("", "Whole_weight", "Shucked_weight", "Length", "Viscera_weight", "Height", "Viscera_weight")
pcr_df <- data.frame(modelno = glancerows_pc$modelno, variable = var_contribution, sigma = glancerows_pc$sigma)
rownames(pcr_df) <- glancerows_pc$variable
knitr::kable(pcr_df)
```

### (c) In a single graph show plots of residual standard deviation resulting from your models on the y -axis against the number of variables/PC components on the x-axis.

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
rsd_df <- data.frame(modelno = glancerows1$modelno, lm = glancerows1$sigma, pcr = pcr_df$sigma)
rsd_df
```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='show'}
rsd_df2 <- tidyr::pivot_longer(rsd_df, -modelno, names_to = "type", values_to = "value")
ggplot(rsd_df2, aes(x = modelno, y = value, color = type, group = type)) +
  geom_point() +
  geom_line() +
  xlab("modelno") +
  ylab("sigma") +
  scale_color_manual(
    labels = c("pcr", "standard lm"),
    values = c("blue", "red")
  ) +
  theme(
    legend.position = c(0.85, 0.5),
    legend.background = element_rect(fill = "white", color = "black")
  )
```

### (d) Explain why you do not require to a variable selection method when selecting the predictors in PCR.

PCR does not require us to choose which predictor variables to add to
the model since each principal component uses a linear combination of
all the predictor variables. Moreover, the principal components are
arranged in descending order of variance, so the first principal
component always has the largest variance, followed by the second and so
on. Hence, a variable selection method is not required as the principal
components can just be added to the model in order of their variances.

### (e) Comment on your findings and in particular on what approaches work better for these data and why.

Based on the correlation heatmap of the predictor variables, we can see
that they are highly collinear. The lines plot above shows that the
standard linear regression has a smaller residual standard deviation
than the principal component regression when the models with only one
variable, but for models with two to four variables, the principal
component regression has smaller residual standard deviations than the
standard linear regression. As for models with five or more variables,
the differences in the residual standard deviations between the two
regressions are either small or zero.

For the standard linear regression, the predictor variables are added to
the model using forward stepwise regression with the largest F-value.
Stepwise regression might not be suitable as the F-statistics do not
have the claimed distribution, and collinearity problems are
exacerbated. For the principal component regression, the predictor
variables are added to the model based on the variances of the principal
components. PCR can perform well even when the predictor variables are
highly collinear, as it produces principal components that are
orthogonal to each other.

In conclusion, for this data set, standard linear regression should be
used for one variable, and principal component regression should be used
for two or more variables.

# Question 3

## We consider the 13-dimensional wine recognition data of Example 4.6 and Lab 4. The data are available in the Data Sets folder. Here we want to compare a factor analysis of all observations with those obtained from cultivar 1 and cultivar 2. The cultivar membership of the observations is given in column 1 of the data set. For part of this analysis you may report the relevant results obtained in the lab. You may find it useful to create two data frames: one for the complete data and a separate one for the first two cultivars of the data. We refer to the latter as the *cultivar12* data. Hint. use the R command factanal from the stats library.

```{r, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
cultivar <- read.table(file = "wine.tsv", sep = ",")

cultivar12 <- cultivar[cultivar$V1 != 3, 2:14]
colnames(cultivar12) = paste0("V",seq(1:13))
cultivar12

cultivar <- cultivar[, 2:14]
colnames(cultivar) = paste0("V",seq(1:13))
cultivar
```

## (a) Scale the data and work with the scaled data. How many observations are in the cultivar12 data?

```{r, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
cultivar_scaled <- scale(cultivar, center = TRUE)

cultivar12_scaled <- scale(cultivar12, center = TRUE)

dim(cultivar12_scaled)
```

There are 130 observations in the cultivar12 data.

## (b) Separately for the complete and for the cultivar12 data, carry out, display and report the results of the following:

### i. Calculate the sample covariance matrix of the scaled data and the eigenvalues of this matrix. What is the value of $\hat{\sigma}^2$ for k=2? How different are the values of $\hat{\sigma}^2$ for the complete and the two cultivar12 datasets? Hint. You may use the information Box 6.7 in your calculations.

```{r}
S1 <- cov(cultivar_scaled)
val1 <- eigen(S1)$values

S2 <- cov(cultivar12_scaled)
val2 <- eigen(S2)$values

sigma_hat_sq1 <- (1 / (13 - 2)) * (sum(val1[3:13]))
sigma_hat_sq2 <- (1 / (13 - 2)) * (sum(val2[3:13]))

sigma_hat_sq1
sigma_hat_sq2
```

The value of $\hat{\sigma}^2$ for the complete dataset is 0.057897 lower
that the one for cultivar12 dataset.

### ii. Calculate and list the factor loadings for the 2-factor principal axis factoring using the value of $\hat{\sigma}^2$ calculated in the previous part.

```{r}
# Factor loading for complete cultivar dataset
Om1 <- diag(rep(sigma_hat_sq1, 13))
S_A1 <- S1 - Om1

eig_A1 <- eigen(S_A1)

Gamma_hat_1 <- eig_A1$vectors[, 1:2]
Lambda_hat_1 <- diag(eig_A1$values[1:2]^(1 / 2))
Ahat1 <- Gamma_hat_1 %*% Lambda_hat_1
Ahat1

# Factor loading for cultivar12 datasets
Om2 <- diag(rep(sigma_hat_sq2, 13))
S_A2 <- S2 - Om2

eig_A2 <- eigen(S_A2)

Gamma_hat_2 <- eig_A2$vectors[, 1:2]
Lambda_hat_2 <- diag(eig_A2$values[1:2]^(1 / 2))
Ahat2 <- Gamma_hat_2 %*% Lambda_hat_2
Ahat2
```

### iii. Show biplots of the factor loadings.

```{r}
wine_pr1 <- prcomp(cultivar_scaled, scale = F)
wine_pr2 <- prcomp(cultivar12_scaled, scale = F)

biplot(wine_pr1$x, Ahat1, col = c("white", "blue"))
biplot(wine_pr2$x, Ahat2, col = c("white", "blue"))

```

### iv. Compare the results obtained from the complete data and the cultivar12 data and comment on the main differences, similarities etc.

The eigenvalues of complete data and cultivar12 data are very similar.
Hence, the values of $\hat{\sigma}^2$ for both data are very similar as
$\hat{\sigma}^2$ is calculated based on the eigenvalues. The
eigenvectors and the factor loadings for the complete data and
cultivar12 data are quite different. The factor loadings for both
dataset differ in terms of absolute value, relative order by size and
the sign. This difference can also be seen in the biplots where the
variables are grouped differently and have different angles.

## (c) We next turn to ML factor loadings and testing. In your calculations use the option "none" for rotation. If you use other commands, you may not achieve full marks for this question. Separately for the complete and for the cultivar12 data, carry out, display and report the results of the following:

### i. Calculate the factor loadings for the 2-factor ML without rotation. List your factor loadings and show biplots of the factor loadings.

```{r}
fa1 <- factanal(cultivar_scaled, factors = 2, rotation = "none")
fa2 <- factanal(cultivar12_scaled, factors = 2, rotation = "none")

fa1$loadings
fa2$loadings

biplot(wine_pr1$x, fa1$loadings, col = c("white", "blue"))
biplot(wine_pr2$x, fa2$loadings, col = c("white", "blue"))
```

### ii. Carry out a sequence of hypothesis tests starting with the one-factor model.

#### A. What is the largest number $\hat{\sigma}^2$ of factors you can test with these data? Why can we not exceed this number?

```{r}
kmax = 1
for (k in 1:13){
  if ((13-k)^2 - (13+k) <= 0){
    break
  }
  kmax = k
}
print(kmax)
```

The largest number $\hat{\sigma}^2$ of factors I can test with these
data is 8. $k$ is used to approximate the degree of freedom $v$ for
chisquare distribution. Since $v = (d-k)^2 - (d+k)$ and $v$ must be
positive because it is not possible to use negative $v$ for chisquare
distribution, $k$ is restricted to a maximum value, which in this case
is 8 to ensure $v$ is positive.

#### B. For each $k \le k_{max}$ , state the number of degrees of freedom of the $\chi^2$ distribution, the limiting distribution of the test statistic $-2log LR_k$ , and report the p-value for each set of tests.

Complete data

```{r}
cultivar_df = data.frame()

cultivar_fa1 = factanal( cultivar_scaled, 1, rotation = "none", scores = "regression" )
wine_row <- data.frame(k = 1, dof = cultivar_fa1$dof, p_value = cultivar_fa1$PVAL)
cultivar_df <- rbind(cultivar_df, wine_row)

cultivar_fa2 = factanal( cultivar_scaled, 2, rotation = "none", scores = "regression" )
wine_row <- data.frame(k = 2, dof = cultivar_fa2$dof, p_value = cultivar_fa2$PVAL)
cultivar_df <- rbind(cultivar_df, wine_row)

cultivar_fa3 = factanal( cultivar_scaled, 3, rotation = "none", scores = "regression" )
wine_row <- data.frame(k = 3, dof = cultivar_fa3$dof, p_value = cultivar_fa3$PVAL)
cultivar_df <- rbind(cultivar_df, wine_row)

cultivar_fa4 = factanal( cultivar_scaled, 4, rotation = "none", scores = "regression" )
wine_row <- data.frame(k = 4, dof = cultivar_fa4$dof, p_value = cultivar_fa4$PVAL)
cultivar_df <- rbind(cultivar_df, wine_row)

cultivar_fa5 = factanal( cultivar_scaled, 5, rotation = "none", scores = "regression" )
wine_row <- data.frame(k = 5, dof = cultivar_fa5$dof, p_value = cultivar_fa5$PVAL)
cultivar_df <- rbind(cultivar_df, wine_row)

cultivar_fa6 = factanal( cultivar_scaled, 6, rotation = "none", scores = "regression" )
wine_row <- data.frame(k = 6, dof = cultivar_fa6$dof, p_value = cultivar_fa6$PVAL)
cultivar_df <- rbind(cultivar_df, wine_row)

cultivar_fa7 = factanal( cultivar_scaled, 7, rotation = "none", scores = "regression" )
wine_row <- data.frame(k = 7, dof = cultivar_fa7$dof, p_value = cultivar_fa7$PVAL)
cultivar_df <- rbind(cultivar_df, wine_row)

cultivar_fa8 = factanal( cultivar_scaled, 8, rotation = "none", scores = "regression" )
wine_row <- data.frame(k = 8, dof = cultivar_fa8$dof, p_value = cultivar_fa8$PVAL)
cultivar_df <- rbind(cultivar_df, wine_row)

rownames(cultivar_df) <- NULL
knitr::kable(cultivar_df)
```

cultivar12 data

```{r}
cultivar12_df <- data.frame()

cultivar12_fa1 = factanal( cultivar12_scaled, 1, rotation = "none", scores = "regression" )
cultivar12_row <- data.frame(k = 1, dof = cultivar12_fa1$dof, p_value = cultivar12_fa1$PVAL)
cultivar12_df <- rbind(cultivar12_df, cultivar12_row)

cultivar12_fa2 = factanal( cultivar12_scaled, 2, rotation = "none", scores = "regression" )
cultivar12_row <- data.frame(k = 2, dof = cultivar12_fa2$dof, p_value = cultivar12_fa2$PVAL)
cultivar12_df <- rbind(cultivar12_df, cultivar12_row)

cultivar12_fa3 = factanal( cultivar12_scaled, 3, rotation = "none", scores = "regression" )
cultivar12_row <- data.frame(k = 3, dof = cultivar12_fa3$dof, p_value = cultivar12_fa3$PVAL)
cultivar12_df <- rbind(cultivar12_df, cultivar12_row)

cultivar12_fa4 = factanal( cultivar12_scaled, 4, rotation = "none", scores = "regression" )
cultivar12_row <- data.frame(k = 4, dof = cultivar12_fa4$dof, p_value = cultivar12_fa4$PVAL)
cultivar12_df <- rbind(cultivar12_df, cultivar12_row)

cultivar12_fa5 = factanal( cultivar12_scaled, 5, rotation = "none", scores = "regression" )
cultivar12_row <- data.frame(k = 5, dof = cultivar12_fa5$dof, p_value = cultivar12_fa5$PVAL)
cultivar12_df <- rbind(cultivar12_df, cultivar12_row)

cultivar12_fa6 = factanal( cultivar12_scaled, 6, rotation = "none", scores = "regression" )
cultivar12_row <- data.frame(k = 6, dof = cultivar12_fa6$dof, p_value = cultivar12_fa6$PVAL)
cultivar12_df <- rbind(cultivar12_df, cultivar12_row)

cultivar12_fa7 = factanal( cultivar12_scaled, 7, rotation = "none", scores = "regression" )
cultivar12_row <- data.frame(k = 7, dof = cultivar12_fa7$dof, p_value = cultivar12_fa7$PVAL)
cultivar12_df <- rbind(cultivar12_df, cultivar12_row)

cultivar12_fa8 = factanal( cultivar12_scaled, 8, rotation = "none", scores = "regression" )
cultivar12_row <- data.frame(k = 8, dof = cultivar12_fa8$dof, p_value = cultivar12_fa8$PVAL)
cultivar12_df <- rbind(cultivar12_df, cultivar12_row)

rownames(cultivar12_df) <- NULL
knitr::kable(cultivar12_df)
```

#### C. What is the appropriate k-factor model for the complete and cultivar12 data?

At k = 5, the hypothesis tests of the complete data and cultivar12 are
smaller than 0.05, which is the significance value, so the suitable
model for both data sets is a 6-factor model. A 5-factor model is ideal
if the significant level is at 0.01.

### iii. Compare the results of parts (b) and (c).

-   If we invert the sign of factor loadings of all data in (b), the
    first factor loadings of all data in (b) agree in sign and order of
    the largest entries well with those of (c). Similarly good agreement
    also exists between the first factor loadings of the cultivar12 data
    in (b) and (c).

-   Biplots of all data in (b) and (c) have the similar variables
    grouped together within small angles e.g.
    (v1, v3, v5, v10, v13) in a group, (v6, v7, v9,v11, v12) in a group
    and (v2, v4 ,v8) in another group.

-   The variables seems to be grouped together similarly between the
    biplots of all data and cultivar12 in (c) whereas the variables
    doesn't seems to be grouped together similarly between the biplots
    of all data and cultivar12 in (b).

-   The biplots of the two ML based loadings in (c) look less similar
    than the biplots of the PC-based loadings of part (b).

-   The biplot of cultivar12 in (b) is very unique from all other
    biplots.

# Question 4

## Consider the Boston Housing data which are available from

library ( MASS ) Boston attach ( Boston ) In Lab 5 we used these data
with the 11 variables shown in Table 7.3 of Chapter 7.

```{r, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
attach(Boston)
Boston
```

(a) Use the split of the 11 variables as in Q3 of Lab 5. Calculate
    canonical correlation scores. List the strength of the four
    correlations and show the four CC score plots corresponding to
    (U•^~j~^, V•~j~ ) for j = 1, . . . , 4.

```{r, out.width="50%"}
Boston.rearranged <- Boston %>% dplyr::select(
  "crim",
  "indus",
  "nox",
  "dis",
  "rad",
  "ptratio",
  "black",
  "rm",
  "age",
  "tax",
  "medv"
)
envsocial <- Boston.rearranged[, 1:7]
individual <- Boston.rearranged[, 8:11]

boston.CC <- cancor(envsocial, individual)

print("The strength of the four correlations: ")
print(boston.CC$cor)

for (i in 1:4) {
  envsocialvals <- as.matrix(envsocial) %*% as.matrix(boston.CC$xcoef[, i])

  individualvals <- as.matrix(individual) %*% as.matrix(boston.CC$ycoef[, i])

  plot(envsocialvals, individualvals)
}
```

(b) Comment on the plots and anything unusual you notice.

The first CC score plot has the unusual property that the data splits
into two separate clusters.\
The second and third CC score plots don't exhibit any interesting
behavior.\
The fourth CC score plot shows very little correlation.s

(c) Use all variables of the Boston Housing data other than chas and add
    the extra variables zn and lstat to the previous X^[2]^ data to
    increase these to 6-dimensional data. Use the X^[1]^ data of part
    (a). Repeat the calculations and graphics of part (a) for these
    data.

```{r, out.width="50%"}
Boston.rearranged <- Boston %>% dplyr::select(
  "crim",
  "indus",
  "nox",
  "dis",
  "rad",
  "ptratio",
  "black",
  "rm",
  "age",
  "tax",
  "medv",
  "zn",
  "lstat"
)
envsocial <- Boston.rearranged[, 1:7]
individual <- Boston.rearranged[, 8:13]

boston.CC <- cancor(envsocial, individual)

print("The strength of the four correlations: ")
print(boston.CC$cor)

for (i in 1:6) {
  envsocialvals <- as.matrix(envsocial) %*% as.matrix(boston.CC$xcoef[, i])

  individualvals <- as.matrix(individual) %*% as.matrix(boston.CC$ycoef[, i])

  plot(envsocialvals, individualvals)
}
```

(d) Compare the results of parts (a) and (c) and comment on the
    differences and why they could occur.

Firstly, the correlation scores are uniformly higher in part (c). This
is likely because the variance of the additional two variables can be
used to find correlations in the X1 data.\
\
There are also now 6 total pairs of CC scores. This is possible because
the rank of the data matrix has increased to 6.

(e) Carry out a hypothesis test for the data described in part (c) using
    the statistic T~k~ of Lecture 5 and the values of the correlation
    strengths obtained in part (c). Calculate the p-values for each
    statistic and report these p-values.

```{r}
Tk <- function(n, d1, d2, cor, k) {
  constant <- n - 0.5 * (d1 + d2 + 3)
  terms <- na.omit(1 - cor[k + 1:length(cor)]^2)
  logterm <- log(prod(terms))
  result <- -constant * logterm
  return(result)
}

n <- 506
d1 <- 7
d2 <- 6
for (k in 1:5) {
  tk <- Tk(n, d1, d2, c(boston.CC$cor), k)
  df <- (d1 - k) * (d2 - k)
  print(paste0("k=", k, " p-value = ", pchisq(tk, df, lower.tail = FALSE)))
}
```

(f) Using a 1% significance level, make a decision regarding the number
    of nonzero correlation coefficients of the population model based on
    your results in part (e).

Looking at the results. At a 1% significance value, we would only retain
the null hypothesis for k=5.

The hypothesis test at k=5 is
$H_0^5: v_1 \ne 0, \dots, v_5 \ne 0, v_6 = 0$ vs
$H_1^5:v_1 \ne 0, \dots, v_6 \ne 0$. Since we fail to reject the null
hypothesis, we assume that the 6th correlation coefficient is zero.

Therefore we conclude that 1,...,5 are nonzero correlation coefficients
(thus there are 5).

(g) Does the decision change if you replace the 1% significance level by
    a 5% significance level? If yes, how? Comment.

Yes, it does change. No decisions would be rejected and therefore we
would assume that there are 6 nonzero correlation coefficients.
