---
title: "Assignment2"
author:
- Michael Nefiodovas(22969312)
- Carmen Leong(22789943)
- Nicholas Choong(21980614)
output:
  pdf_document: default
  html_notebook: default
  word_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include=FALSE}
setwd("~/Library/CloudStorage/OneDrive-TheUniversityofWesternAustralia/STAT3064/Data Sets")
#setwd("C:/Users/user/OneDrive - The University of Western Australia/Units/STAT3064/Labs/STAT3064-Ass2")

rm(list = ls())

if (!is.null(sessionInfo()$otherPkgs)) {
  invisible(
    lapply(paste0("package:", names(sessionInfo()$otherPkgs)),
      detach,
      character.only = TRUE, unload = TRUE
    )
  )
}

options(stringsAsFactors = FALSE)
```

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
library(reshape2)
library(tidyverse)
library(MASS)
library(GGally)
library(mvtnorm)
library(scales)
library(ggpubr)
library(dplyr)
library(factoextra)
library(broom)
library(pls)
```

# Question 1

## (a) Why should you not automatically scale the data prior to a PCA or FA? Restrict your answer to one or two concise sentences.

## (b) The dataset ass2pop.csv is available in the LMS folder 'Data sets'. For a description of the data see Assignment 1. Here we work with a part of the dataset only.

Let Σ be the covariance matrix consisting of rows 1:11, and columns
3:13. Read the data into R. The value for Σ[1, 1] should be 0.8266. In
your answer show the R commands you use to calculate the following and
show the results stating clearly what each part is.

### i. the eigenvalues of Σ;

ii. the matrix Σ^2/3^;
iii. the matrix 2Σ^−1/4^ΣΣ^−1/4^ and its eigenvalues

# Question 2

## Consider the abalone data. We want to compare the performance of linear regression and PCR for the raw abalone data following the description given in Q3 of Lab 3. In the analysis we use the predictor variables Length, Height, Whole Weight, Shucked Weight, Viscera Weight and Dried-Shell Weight and we consider Rings as the response variable. Hint. Note the change of predictor variables used in Q2 compared to the variables in the Lab.

```{r}
coln <- c(
  "Sex", # 		nominal			M, F, and I (infant)
  "Length", # 		continuous	mm	Longest shell measurement
  "Diameter", # 	continuous	mm	perpendicular to length
  "Height", # 		continuous	mm	with meat in shell
  "Whole_weight", # 	continuous	grams	whole abalone
  "Shucked_weight", # 	continuous	grams	weight of meat
  "Viscera_weight", # 	continuous	grams	gut weight (after bleeding)
  "Shell_weight", # 	continuous	grams	after being dried
  "Rings" # 		integer			+1.5 gives the age in years
)
abalone <- read_csv(file = "./abalone.csv", col_names = coln)
summary(abalone)
summary
```

```{r}
# Regression
big.lm <- lm(Rings ~ Length + Height + Whole_weight + Shucked_weight + Viscera_weight + Shell_weight, data = abalone)
summary(big.lm)
```

### (a) For the regular linear regression use forward selection and state the order in which the variables are chosen. Calculate the residual standard deviation for each number of predictors. Hint. you may make use of the code in Lab 3.

```{r}
glancerows <- data.frame()
fm.fwd <- fm.null <- lm(Rings ~ 1, abalone)
summary(lm(fm.fwd))
row1 <- data.frame(modelno = 0, variable = "Intercept", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r}
fm.fwd <- update(fm.fwd, . ~ . + Shell_weight)
row1 <- data.frame(modelno = 1, variable = "Shell_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```
```{r}
fm.fwd <- update(fm.fwd, . ~ . + Shucked_weight)
row1 <- data.frame(modelno = 2, variable = "Shucked_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r}
fm.fwd <- update(fm.fwd, . ~ . + Length)
row1 <- data.frame(modelno = 3, variable = "Length", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```
```{r}
fm.fwd <- update(fm.fwd, . ~ . + Whole_weight)
row1 <- data.frame(modelno = 4, variable = "Whole_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r}
fm.fwd <- update(fm.fwd, . ~ . + Height)
row1 <- data.frame(modelno = 5, variable = "Height", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r}
fm.fwd <- update(fm.fwd, . ~ . + Viscera_weight)
row1 <- data.frame(modelno = 6, variable = "Viscera_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
```

```{r}
glancerows
```

Shell_weight, Shucked_weight, Length, Whole_weight, Height, Viscera_weight

### (b) Carry out PCR on the raw data using the same variables and response as in part (a). For each additional principal component you add to the regression model as predictor, calculate the residual standard deviation and list which of the variables has the heighest absolute weight in the respective principal component.

```{r}
pcr_model1 <- pcr(Rings ~ Whole_weight, data = abalone, scale = F, validation = "CV")
summary(pcr_model1)
pcr_model2 <- pcr(Rings ~ Whole_weight + Shucked_weight, data = abalone, scale = F, validation = "CV")
summary(pcr_model2)
pcr_model3 <- pcr(Rings ~ Whole_weight + Shucked_weight + Length, data = abalone, scale = F, validation = "CV")
summary(pcr_model3)
pcr_model4 <- pcr(Rings ~ Whole_weight + Shucked_weight + Length + Shell_weight, data = abalone, scale = F, validation = "CV")
summary(pcr_model4)
pcr_model5 <- pcr(Rings ~ Whole_weight + Shucked_weight + Length + Shell_weight + Height, data = abalone, scale = F, validation = "CV")
summary(pcr_model5)
pcr_model6 <- pcr(Rings ~ Whole_weight + Shucked_weight + Length + Shell_weight + Height + Viscera_weight, data = abalone, scale = F, validation = "CV")
summary(pcr_model6)
pcr_model7 <- pcr(Rings ~ Shell_weight + Shucked_weight + Length + Whole_weight + Height + Viscera_weight, data = abalone, scale = F, validation = "CV")
summary(pcr_model7)
```


### (c) In a single graph show plots of residual standard deviation resulting from your models on the y -axis against the number of variables/PC components on the x-axis.

```{r}
rsd_df <- data.frame(RMSEP(pcr_model6)$val[1,,])
colnames(rsd_df) <- c("pcr")
rsd_df$modelno <- glancerows$modelno
rsd_df$lm <- glancerows$sigma
rsd_df
```

```{r}
rsd_df2 <- tidyr::pivot_longer(rsd_df, -modelno, names_to = "type", values_to = "value")
ggplot(rsd_df2, aes(x = modelno, y = value, color = type, group = type)) +
  geom_point() +
  geom_line() +
  xlab("modelno") +
  ylab("sigma") +
  scale_color_manual(labels = c("pcr", "lm")
                     ,values = c("blue", "red")) +
  theme(legend.position = c(0.85, 0.5),
        legend.background = element_rect(fill = "white", color = "black"))
```


### (d) Explain why you do not require to a variable selection method when selecting the predictors in PCR.

### (e) Comment on your findings and in particular on what approaches work better for these data and why.

# Question 3

## We consider the 13-dimensional wine recognition data of Example 4.6 and Lab 4. The data are available in the Data Sets folder. Here we want to compare a factor analysis of all observations with those obtained from cultivar 1 and cultivar 2. The cultivar membership of the observations is given in column 1 of the data set. For part of this analysis you may report the relevant results obtained in the lab. You may find it useful to create two data frames: one for the complete data and a separate one for the first two cultivars of the data. We refer to the latter as the *cultivar12* data. Hint. use the R command factanal from the stats library.

## (a) Scale the data and work with the scaled data. How many

observations are in the cultivar12 data?

## (b) Separately for the complete and for the cultivar12 data, carry

out, display and report the results of the following:

### i. Calculate the sample covariance matrix of the scaled data and the eigenvalues of this matrix. What is the value of $\hat{σ}^2$ for k=2? How different are the values of $\hat{σ}^2$ for the complete and the two cultivar12 datasets? Hint. You may use the information Box 6.7 in your calculations.

### ii. Calculate and list the factor loadings for the 2-factor principal axis factoring using the value of $\hat{σ}^2$ calculated in the previous part.

### iii. Show biplots of the factor loadings.

### iv. Compare the results obtained from the complete data and the cultivar12 data and comment on the main differences, similarities etc.

## (c) We next turn to ML factor loadings and testing. In your calculations use the option "none" for rotation. If you use other commands, you may not achieve full marks for this question. Separately for the complete and for the cultivar12 data, carry out, display and report the results of the following:

### i. Calculate the factor loadings for the 2-factor ML without rotation. List your factor loadings and show biplots of the factor loadings.

### ii. Carry out a sequence of hypothesis tests starting with the one-factor model.

#### A. What is the largest number $\hat{σ}^2$ of factors you can test with these data? Why can we not exceed this number?

#### B. For each k ≤ k^~max~^ , state the number of degrees of freedom of the χ^2^ distribution, the limiting distribution of the tes statistic −2 log LR~k~ , and report the p-value for each set of tests.

#### C. What is the appropriate k-factor model for the complete and cultivar12 data?

### iii. Compare the results of parts (b) and (c).

# Question 4

## Consider the Boston Housing data which are available from

library ( MASS ) Boston attach ( Boston ) In Lab 5 we used these data
with the 11 variables shown in Table 7.3 of Chapter 7.

## (a) Use the split of the 11 variables as in Q3 of Lab 5. Calculate canonical correlation scores. List the strength of the four correlations and show the four CC score plots corresponding to (U•^~j~^, V•~j~ ) for j = 1, . . . , 4.

## (b) Comment on the plots and anything unusual you notice.

## (c) Use all variables of the Boston Housing data other than chas and add the extra variables zn and lstat to the previous X^[2]^ data to increase these to 6-dimensional data. Use the X^[1]^ data of part (a). Repeat the calculations and graphics of part (a) for these data.

## (d) Compare the results of parts (a) and (c) and comment on thedifferences and why they could occur.

## (e) Carry out a hypothesis test for the data described in part (c) using the statistic T~k~ of Lecture 5 and the values of the correlation strengths obtained in part (c). Calculate the p-values for each statistic and report these p-values.

## (f) Using a 1% significance level, make a decision regarding the number of nonzero correlation coefficients of the population model based on your results in part (e).

## (g) Does the decision change if you replace the 1% significance level by a 5% significance level? If yes, how? Comment.
