---
title: "Assignment2"
author:
- Michael Nefiodovas(22969312)
- Carmen Leong(22789943)
- Nicholas Choong(21980614)
output:
  pdf_document: default
  html_notebook: default
  word_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include=FALSE}
#setwd("~/Library/CloudStorage/OneDrive-TheUniversityofWesternAustralia/STAT3064/Data Sets")
setwd("C:/Users/user/OneDrive - The University of Western Australia/Units/STAT3064/Labs/STAT3064-Ass2")

rm(list = ls())

if (!is.null(sessionInfo()$otherPkgs)) {
  invisible(
    lapply(paste0("package:", names(sessionInfo()$otherPkgs)),
      detach,
      character.only = TRUE, unload = TRUE
    )
  )
}

options(stringsAsFactors = FALSE)
```

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'}
library(reshape2)
library(tidyverse)
library(MASS)
library(GGally)
library(mvtnorm)
library(scales)
library(ggpubr)
library(dplyr)
library(factoextra)
library(broom)
library(pls)
```

# Question 1

## (a) Why should you not automatically scale the data prior to a PCA or FA? Restrict your answer to one or two concise sentences.

Scaling can possibly cause information loss especially for variables
with high magnitude. If the variables with high magnitude are important,
scaling is not advised since scaling centres all the variables and
transforms their variability and range to more comparable ranges.
Moreover, if the variables in the data set have same units of
measurement, scaling is not necessarily required.

## (b) The dataset ass2pop.csv is available in the LMS folder 'Data sets'. For a description of the data see Assignment 1. Here we work with a part of the dataset only. Let Σ be the covariance matrix consisting of rows 1:11, and columns 3:13. Read the data into R. The value for Σ[1, 1] should be 0.8266. In your answer show the R commands you use to calculate the following and show the results stating clearly what each part is.

```{r}
ass2pop <- read.csv("ass2pop.csv", header = FALSE)
S0 = as.matrix(ass2pop[1:11,3:13])
S0
```

The covariance matrix is a 11x11 square matrix giving the covariance
between each pair of the first 11 variables from the first population.

### i. the eigenvalues of Σ;

```{r}
ev = eigen(S0)
vals = ev$values
vals
```

The eigenvalues of the covariance matrix encode the variability of the
data in an orthogonal basis that captures as much of the data's
variability as possible.

### ii. the matrix Σ^2/3^;

```{r}
V1 = ev$vectors
vals2 = diag(vals^(2/3) )
S1 = V1 %*% vals2 %*% t( V1 )
S1
```

### iii. the matrix 2Σ^−1/4^ΣΣ^−1/4^ and its eigenvalues

```{r}
vals3 = diag(vals^(-1/4) )
S2 = V1 %*% vals3 %*% t( V1 )
mat = 2*S2%*%S0%*%S2
mat
vals4 = eigen(mat)$values
vals4
```

# Question 2

## Consider the abalone data. We want to compare the performance of linear regression and PCR for the raw abalone data following the description given in Q3 of Lab 3. In the analysis we use the predictor variables Length, Height, Whole Weight, Shucked Weight, Viscera Weight and Dried-Shell Weight and we consider Rings as the response variable. Hint. Note the change of predictor variables used in Q2 compared to the variables in the Lab.

```{r}
coln <- c(
  "Sex", # 		nominal			M, F, and I (infant)
  "Length", # 		continuous	mm	Longest shell measurement
  "Diameter", # 	continuous	mm	perpendicular to length
  "Height", # 		continuous	mm	with meat in shell
  "Whole_weight", # 	continuous	grams	whole abalone
  "Shucked_weight", # 	continuous	grams	weight of meat
  "Viscera_weight", # 	continuous	grams	gut weight (after bleeding)
  "Shell_weight", # 	continuous	grams	after being dried
  "Rings" # 		integer			+1.5 gives the age in years
)
abalone <- read_csv(file = "./abalone.csv", col_names = coln)
summary(abalone)
summary
```

```{r}
# Regression
big.lm <- lm(Rings ~ Length + Height + Whole_weight + Shucked_weight + Viscera_weight + Shell_weight, data = abalone)
summary(big.lm)
```

### (a) For the regular linear regression use forward selection and state the order in which the variables are chosen. Calculate the residual standard deviation for each number of predictors. Hint. you may make use of the code in Lab 3.

```{r}
glancerows <- data.frame()
fm.fwd <- fm.null <- lm(Rings ~ 1, abalone)
summary(lm(fm.fwd))
row1 <- data.frame(modelno = 0, variable = "Intercept", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r}
fm.fwd <- update(fm.fwd, . ~ . + Shell_weight)
row1 <- data.frame(modelno = 1, variable = "Shell_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r}
fm.fwd <- update(fm.fwd, . ~ . + Shucked_weight)
row1 <- data.frame(modelno = 2, variable = "Shucked_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r}
fm.fwd <- update(fm.fwd, . ~ . + Length)
row1 <- data.frame(modelno = 3, variable = "Length", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r}
fm.fwd <- update(fm.fwd, . ~ . + Whole_weight)
row1 <- data.frame(modelno = 4, variable = "Whole_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r}
fm.fwd <- update(fm.fwd, . ~ . + Height)
row1 <- data.frame(modelno = 5, variable = "Height", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
add1(fm.fwd, big.lm, test = "F")
```

```{r}
fm.fwd <- update(fm.fwd, . ~ . + Viscera_weight)
row1 <- data.frame(modelno = 6, variable = "Viscera_weight", sigma = glance(lm(fm.fwd))$sigma)
glancerows <- rbind(glancerows, row1)
```

```{r}
glancerows
```

Shell_weight, Shucked_weight, Length, Whole_weight, Height,
Viscera_weight

### (b) Carry out PCR on the raw data using the same variables and response as in part (a). For each additional principal component you add to the regression model as predictor, calculate the residual standard deviation and list which of the variables has the heighest absolute weight in the respective principal component.

```{r}
pcr_model1 <- pcr(Rings ~ Whole_weight, data = abalone, scale = F, validation = "CV")
summary(pcr_model1)
pcr_model2 <- pcr(Rings ~ Whole_weight + Shucked_weight, data = abalone, scale = F, validation = "CV")
summary(pcr_model2)
pcr_model3 <- pcr(Rings ~ Whole_weight + Shucked_weight + Length, data = abalone, scale = F, validation = "CV")
summary(pcr_model3)
pcr_model4 <- pcr(Rings ~ Whole_weight + Shucked_weight + Length + Shell_weight, data = abalone, scale = F, validation = "CV")
summary(pcr_model4)
pcr_model5 <- pcr(Rings ~ Whole_weight + Shucked_weight + Length + Shell_weight + Height, data = abalone, scale = F, validation = "CV")
summary(pcr_model5)
pcr_model6 <- pcr(Rings ~ Whole_weight + Shucked_weight + Length + Shell_weight + Height + Viscera_weight, data = abalone, scale = F, validation = "CV")
summary(pcr_model6)
pcr_model7 <- pcr(Rings ~ Shell_weight + Shucked_weight + Length + Whole_weight + Height + Viscera_weight, data = abalone, scale = F, validation = "CV")
summary(pcr_model7)
```

### (c) In a single graph show plots of residual standard deviation resulting from your models on the y -axis against the number of variables/PC components on the x-axis.

```{r}
rsd_df <- data.frame(RMSEP(pcr_model6)$val[1,,])
colnames(rsd_df) <- c("pcr")
rsd_df$modelno <- glancerows$modelno
rsd_df$lm <- glancerows$sigma
rsd_df
```

```{r}
rsd_df2 <- tidyr::pivot_longer(rsd_df, -modelno, names_to = "type", values_to = "value")
ggplot(rsd_df2, aes(x = modelno, y = value, color = type, group = type)) +
  geom_point() +
  geom_line() +
  xlab("modelno") +
  ylab("sigma") +
  scale_color_manual(labels = c("pcr", "lm")
                     ,values = c("blue", "red")) +
  theme(legend.position = c(0.85, 0.5),
        legend.background = element_rect(fill = "white", color = "black"))
```

### (d) Explain why you do not require to a variable selection method when selecting the predictors in PCR.

### (e) Comment on your findings and in particular on what approaches work better for these data and why.

# Question 3

## We consider the 13-dimensional wine recognition data of Example 4.6 and Lab 4. The data are available in the Data Sets folder. Here we want to compare a factor analysis of all observations with those obtained from cultivar 1 and cultivar 2. The cultivar membership of the observations is given in column 1 of the data set. For part of this analysis you may report the relevant results obtained in the lab. You may find it useful to create two data frames: one for the complete data and a separate one for the first two cultivars of the data. We refer to the latter as the *cultivar12* data. Hint. use the R command factanal from the stats library.

```{r}
cultivar = read.table(file = 'wine.tsv', sep = ',')

cultivar12 = cultivar[cultivar$V1 != 3, 2:14]
cultivar12

cultivar = cultivar[,2:14]
cultivar
```

## (a) Scale the data and work with the scaled data. How many observations are in the cultivar12 data?

```{r}
cultivar_scaled = scale(cultivar, center = TRUE)
cultivar_scaled

cultivar12_scaled = scale(cultivar12, center = TRUE)
cultivar12_scaled
```

There are 130 observations in the cultivar12 data.

## (b) Separately for the complete and for the cultivar12 data, carry out, display and report the results of the following:

### i. Calculate the sample covariance matrix of the scaled data and the eigenvalues of this matrix. What is the value of $\hat{σ}^2$ for k=2? How different are the values of $\hat{σ}^2$ for the complete and the two cultivar12 datasets? Hint. You may use the information Box 6.7 in your calculations.

```{r}
S1 = cov(cultivar_scaled)
val1 = eigen(S1)$values
val1

S2 = cov(cultivar12_scaled)
val2 = eigen(S2)$values
val2

sigma_hat_sq1 = (1/(13-2))*(sum(val1[3:13]))
sigma_hat_sq2 = (1/(13-2))*(sum(val2[3:13]))

sigma_hat_sq1
sigma_hat_sq2
```

The value of $\hat{σ}^2$ for the complete dataset is 0.57897 lower that
the one for cultivar12 dataset.

### ii. Calculate and list the factor loadings for the 2-factor principal axis factoring using the value of $\hat{σ}^2$ calculated in the previous part.

```{r}
# Factor loading for complete cultivar dataset
Om1 = diag( rep( sigma_hat_sq1, 13 ) ) 
S_A1 = S1 - Om1

eig_A1 = eigen( S_A1 )

Gamma_hat_1 = eig_A1$vectors[ ,1:2]
Lambda_hat_1 =  diag( eig_A1$values[1:2]^(1/2) )
Ahat1 = Gamma_hat_1 %*% Lambda_hat_1  
Ahat1

# Factor loading for cultivar12 datasets
Om2 = diag( rep( sigma_hat_sq2, 13 ) ) 
S_A2 = S2 - Om2

eig_A2 = eigen( S_A2 )

Gamma_hat_2 = eig_A2$vectors[ ,1:2]
Lambda_hat_2 =  diag( eig_A2$values[1:2]^(1/2) )
Ahat2 = Gamma_hat_2%*% Lambda_hat_2 
Ahat2

```

### iii. Show biplots of the factor loadings.

```{r}
wine_pr1 <- prcomp(cultivar_scaled, scale = F)
wine_pr2 <- prcomp(cultivar12_scaled, scale = F)

biplot( wine_pr1$x, Ahat1, col = c("white", "blue") )
biplot( wine_pr2$x, Ahat2, col = c("white", "blue") )
```

### iv. Compare the results obtained from the complete data and the cultivar12 data and comment on the main differences, similarities etc.

The eigenvalues of complete data and cultivar12 data are very similar.
Hence, the values of $\hat{σ}^2$ for both data are very similar as
$\hat{σ}^2$ is calculated based on the eigenvalues.
The eigenvectors and the factor loadings for the complete data and
cultivar12 data are quite different. The factor loadings for both
dataset differ in terms of absolute value, relative order by
size and the sign. This difference can also be seen in the biplots where
the variables are grouped differently and have different angles.

## (c) We next turn to ML factor loadings and testing. In your calculations use the option "none" for rotation. If you use other commands, you may not achieve full marks for this question. Separately for the complete and for the cultivar12 data, carry out, display and report the results of the following:

### i. Calculate the factor loadings for the 2-factor ML without rotation. List your factor loadings and show biplots of the factor loadings.

```{r}
fa1 = factanal(cultivar_scaled, factors = 2, rotation = "none")$loadings
fa2 = factanal(cultivar12_scaled, factors = 2, rotation = "none")$loadings

fa1
fa2

biplot( wine_pr1$x, fa1, col = c("white", "blue") )
biplot( wine_pr1$x, fa2, col = c("white", "blue") )
```

### ii. Carry out a sequence of hypothesis tests starting with the one-factor model.

#### A. What is the largest number $\hat{σ}^2$ of factors you can test with these data? Why can we not exceed this number?

#### B. For each k ≤ k^~max~^ , state the number of degrees of freedom of the χ^2^ distribution, the limiting distribution of the tes statistic −2 log LR~k~ , and report the p-value for each set of tests.

#### C. What is the appropriate k-factor model for the complete and cultivar12 data?

### iii. Compare the results of parts (b) and (c).

# Question 4

## Consider the Boston Housing data which are available from

library ( MASS ) Boston attach ( Boston ) In Lab 5 we used these data
with the 11 variables shown in Table 7.3 of Chapter 7.

```{r}
attach(Boston)
Boston
```

(a) Use the split of the 11 variables as in Q3 of Lab 5. Calculate
    canonical correlation scores. List the strength of the four
    correlations and show the four CC score plots corresponding to
    (U•^~j~^, V•~j~ ) for j = 1, . . . , 4.

```{r}
Boston.rearranged <- Boston %>% dplyr::select("crim", "indus", "nox", "dis", "rad", "ptratio", "black", "rm", "age", "tax", "medv")
envsocial <- Boston.rearranged[, 1:7]
individual <- Boston.rearranged[, 8:11]

boston.CC <- cancor(envsocial, individual)

print("The strength of the four correlations: ")
print(boston.CC$cor)

for (i in 1:4) {
  envsocialvals <- as.matrix(envsocial) %*% as.matrix(boston.CC$xcoef[, i])

  individualvals <- as.matrix(individual) %*% as.matrix(boston.CC$ycoef[, i])

  plot(envsocialvals, individualvals)  
}

```

(b) Comment on the plots and anything unusual you notice.

The first CC score plot has the unusual property that the data splits
into two separate clusters.\
The second and third CC score plots don't exhibit any interesting
behavior.\
The fourth CC score plot shows very little correlation.s

c\. Use all variables of the Boston Housing data other than chas and add
the extra variables zn and lstat to the previous X^[2]^ data to increase
these to 6-dimensional data. Use the X^[1]^ data of part (a). Repeat the
calculations and graphics of part (a) for these data.

```{r}
Boston.rearranged <- Boston %>% dplyr::select("crim", "indus", "nox", "dis", "rad", "ptratio", "black", "rm", "age", "tax", "medv", "zn", "lstat")
envsocial <- Boston.rearranged[, 1:7]
individual <- Boston.rearranged[, 8:13]

boston.CC <- cancor(envsocial, individual)

print("The strength of the four correlations: ")
print(boston.CC$cor)

for (i in 1:6) {
  envsocialvals <- as.matrix(envsocial) %*% as.matrix(boston.CC$xcoef[, i])

  individualvals <- as.matrix(individual) %*% as.matrix(boston.CC$ycoef[, i])

  plot(envsocialvals, individualvals)  
}

```

d\. Compare the results of parts (a) and (c) and comment on the
differences and why they could occur.

Firstly, the correlation scores are uniformly higher in part (c). This
is likely because the variance of the additional two variables can be
used to find correlations in the X1 data.\
\
There are also now 6 total pairs of CC scores. This is possible because
the rank of the data matrix has increased to 6.

e\. Carry out a hypothesis test for the data described in part (c) using
the statistic T~k~ of Lecture 5 and the values of the correlation
strengths obtained in part (c). Calculate the p-values for each
statistic and report these p-values.

```{r}
Tk <- function(n, d1, d2, cor, k) {
  constant <- n - 0.5 * (d1 + d2 + 3)
  terms <- na.omit(1 - cor[k + 1:length(cor)]^2)
  logterm <- log(prod(terms))
  result <- - constant * logterm
  return(result)
}

n <- 506
d1 <- 7
d2 <- 6
for (k in 1:5) {
  tk <- Tk(n, d1, d2, c(boston.CC$cor), k)
  df <- (d1 - k) * (d2 - k)
  print(paste0("k=" , k, " p-value = ", pchisq(tk, df, lower.tail = FALSE)))
}
```

f\. Using a 1% significance level, make a decision regarding the number
of nonzero correlation coefficients of the population model based on
your results in part (e).

Looking at the results. At a 1% significance value, we would only retain
the null hypothesis for k=5.

The hypothesis test at k=5 is
$H_0^5: v_1 \ne 0, \dots, v_5 \ne 0, v_6 = 0$ vs
$H_1^5:v_1 \ne 0, \dots, v_6 \ne 0$. Since we fail to reject the null
hypothesis, we assume that the 6th correlation coefficient is zero.

Therefore we conclude that 1,...,5 are nonzero correlation coefficients
(thus there are 5).

g\. Does the decision change if you replace the 1% significance level by
a 5% significance level? If yes, how? Comment.

Yes, it does change. No decisions would be rejected and therefore we
would assume that there are 6 nonzero correlation coefficients.
